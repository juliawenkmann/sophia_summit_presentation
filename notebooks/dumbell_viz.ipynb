{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rp4pVRTJxcbv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliawenkmann/miniconda3/envs/ml/lib/python3.11/site-packages/torchvision/models/googlenet.py:341: UserWarning: auxiliary heads in the pretrained googlenet model are NOT pretrained, so make sure to train them\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import math, random, numpy as np\n",
    "import torch, torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "# ---- 1) Model (GoogLeNet ~ Inception-family) ----\n",
    "weights = models.GoogLeNet_Weights.IMAGENET1K_V1\n",
    "net = models.googlenet(weights=weights, aux_logits=True).to(device).eval()\n",
    "for p in net.parameters(): p.requires_grad_(False)\n",
    "\n",
    "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406], device=device)[:, None, None]\n",
    "IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225], device=device)[:, None, None]\n",
    "CATEGORIES = weights.meta[\"categories\"]  # list of 1000 labels ordered by index\n",
    "\n",
    "def class_idx_from_name(name, categories=CATEGORIES):\n",
    "    name = name.lower()\n",
    "    # simple fuzzy match; first exact contains, then fallback\n",
    "    try:\n",
    "        return categories.index(name)\n",
    "    except ValueError:\n",
    "        hits = [i for i, s in enumerate(categories) if name in s.lower()]\n",
    "        if not hits:\n",
    "            raise ValueError(f\"'{name}' not found in ImageNet categories.\")\n",
    "        return hits[0]\n",
    "\n",
    "# ---- 2) Fourier parameterization helpers ----\n",
    "def radial_freq(h, w, device):\n",
    "    fy = torch.fft.fftfreq(h, d=1.0).to(device).reshape(h, 1)\n",
    "    fx = torch.fft.rfftfreq(w, d=1.0).to(device).reshape(1, w//2 + 1)\n",
    "    return torch.sqrt(fx*fx + fy*fy).clamp(min=1e-6)\n",
    "\n",
    "def make_fft_params(h=384, w=384):\n",
    "    # Real/imag parts for rFFT spectrum\n",
    "    spec = torch.randn(1, 3, h, w//2 + 1, 2, device=device, requires_grad=True)\n",
    "    freqs = radial_freq(h, w, device)\n",
    "    return spec, freqs\n",
    "\n",
    "def spectrum_to_image(spec, freqs, decay_power=1.5):\n",
    "    \"\"\"Inverse FFT with 1/f^decay frequency decay -> natural image bias, map to [0,1].\"\"\"\n",
    "    # shape: [1,3,H,W//2+1,2] -> complex\n",
    "    complex_spec = torch.view_as_complex(spec)\n",
    "    scaled = complex_spec * (1.0 / (freqs ** decay_power))  # low-freq emphasis\n",
    "    img = torch.fft.irfft2(scaled, s=(spec.shape[2], (spec.shape[3]-1)*2), norm='ortho')\n",
    "    img = img / (img.std(dim=(-2, -1), keepdim=True) + 1e-8)   # stabilize contrast\n",
    "    img = torch.tanh(img) * 0.5 + 0.5                          # [0,1] with gradient\n",
    "    return img\n",
    "\n",
    "# ---- 3) Priors ----\n",
    "def tv_loss(x):\n",
    "    dx = x[..., 1:] - x[..., :-1]\n",
    "    dy = x[..., :, 1:, :] - x[..., :, :-1, :]\n",
    "    return (dx.pow(2).mean() + dy.pow(2).mean())\n",
    "\n",
    "def blur3(x):\n",
    "    # simple 3x3 box blur to suppress ringing\n",
    "    return F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "# ---- 4) Data augmentation for transform robustness ----\n",
    "def random_view(x, out_hw=(224, 224), jitter=24, rot=15, scale_range=(0.92, 1.08), flip=True, noise_std=0.02):\n",
    "    # noise_std= 0.006\n",
    "    # x: [1,3,H,W]\n",
    "    _, _, H, W = x.shape\n",
    "    # jitter via integer roll\n",
    "    ox, oy = np.random.randint(-jitter, jitter+1, 2)\n",
    "    v = torch.roll(x, shifts=(ox, oy), dims=(-1, -2))\n",
    "    # small affine\n",
    "    angle = float(random.uniform(-rot, rot))\n",
    "    scale = float(random.uniform(*scale_range))\n",
    "    trans = (int(random.uniform(-0.06*W, 0.06*W)), int(random.uniform(-0.06*H, 0.06*H)))\n",
    "    v = TF.affine(v, angle=angle, translate=trans, scale=scale, shear=[0.0, 0.0])\n",
    "    if flip and random.random() < 0.5:\n",
    "        v = TF.hflip(v)\n",
    "    # resize to model’s resolution\n",
    "    v = F.interpolate(v, size=out_hw, mode='bilinear', align_corners=False)\n",
    "    # light noise\n",
    "    if noise_std > 0:\n",
    "        v = (v + noise_std * torch.randn_like(v)).clamp(0, 1)\n",
    "    # normalize\n",
    "    v = (v - IMAGENET_MEAN) / IMAGENET_STD\n",
    "    return v\n",
    "\n",
    "# ---- 5) Main optimization ----\n",
    "@torch.no_grad()\n",
    "def to_pil(x):\n",
    "    x = x.squeeze(0).clamp(0,1).cpu()\n",
    "    return TF.to_pil_image(x)\n",
    "\n",
    "def synthesize_class(\n",
    "    class_name=\"dumbbell\",\n",
    "    steps=700,\n",
    "    n_views=8,\n",
    "    size=384,\n",
    "    lr=0.08,\n",
    "    tv_w=1e-4,\n",
    "    l2_w=1e-6,\n",
    "    decay_power=1.5,\n",
    "    blur_every=60\n",
    "):\n",
    "    target = class_idx_from_name(class_name)  # e.g., \"dumbbell\" -> correct ImageNet index\n",
    "    # print(target)\n",
    "    spec, freqs = make_fft_params(size, size)\n",
    "    opt = torch.optim.Adam([spec], lr=lr)\n",
    "\n",
    "    for t in range(steps):\n",
    "        img = spectrum_to_image(spec, freqs, decay_power=decay_power)\n",
    "\n",
    "        if blur_every and t > 0 and t % blur_every == 0:\n",
    "            img = blur3(img)\n",
    "\n",
    "        # build a batch of randomly transformed views\n",
    "        batch = torch.cat([random_view(img) for _ in range(n_views)], dim=0)\n",
    "        logits = net(batch)\n",
    "        cls = logits[:, target].mean()\n",
    "\n",
    "        # natural image priors\n",
    "        tv = tv_loss(img)\n",
    "        l2 = ((img - 0.5) ** 2).mean()\n",
    "\n",
    "        # maximize class score with priors\n",
    "        loss = cls - tv_w * tv - l2_w * l2\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        (-loss).backward()   # gradient ascent\n",
    "        opt.step()\n",
    "\n",
    "        # gentle spectrum damping prevents exploding amplitudes\n",
    "        with torch.no_grad():\n",
    "            spec.mul_(0.995)\n",
    "\n",
    "    final = spectrum_to_image(spec, freqs, decay_power=decay_power)\n",
    "    return to_pil(final)\n",
    "\n",
    "# ---- Example:\n",
    "# out = synthesize_class(\"dumbbell\", steps=100, n_views=8, size=448, lr=0.01, decay_power=1.7)\n",
    "out = synthesize_class(\"dumbbell\", steps=1_000, n_views=8, size=448, lr=0.01, decay_power=0.5)\n",
    "out.save(\"dumbbell_classviz.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7jzLT5xxlAn"
   },
   "outputs": [],
   "source": [
    "import math, random, numpy as np\n",
    "import torch, torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ===== Model (GoogLeNet / Inception-family) =====\n",
    "weights = models.GoogLeNet_Weights.IMAGENET1K_V1\n",
    "net = models.googlenet(weights=weights, aux_logits=True).to(device).eval()\n",
    "for p in net.parameters(): p.requires_grad_(False)\n",
    "\n",
    "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406], device=device)[:, None, None]\n",
    "IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225], device=device)[:, None, None]\n",
    "CATEGORIES = weights.meta[\"categories\"]\n",
    "\n",
    "def class_idx_from_name(name, categories=CATEGORIES):\n",
    "    name = name.lower()\n",
    "    try:\n",
    "        return categories.index(name)\n",
    "    except ValueError:\n",
    "        hits = [i for i, s in enumerate(categories) if name in s.lower()]\n",
    "        if not hits: raise ValueError(f\"'{name}' not found in ImageNet categories.\")\n",
    "        return hits[0]\n",
    "\n",
    "# ===== Fourier parameterization =====\n",
    "def radial_freq(h, w, device):\n",
    "    fy = torch.fft.fftfreq(h, d=1.0).to(device).reshape(h, 1)\n",
    "    fx = torch.fft.rfftfreq(w, d=1.0).to(device).reshape(1, w//2 + 1)\n",
    "    return torch.sqrt(fx*fx + fy*fy).clamp(min=1e-6)\n",
    "\n",
    "def make_fft_params(h=448, w=448):\n",
    "    spec = torch.randn(1, 3, h, w//2 + 1, 2, device=device, requires_grad=True)\n",
    "    freqs = radial_freq(h, w, device)\n",
    "    return spec, freqs\n",
    "\n",
    "def spectrum_to_image(spec, freqs, decay_power=1.8, contrast=1.8):\n",
    "    complex_spec = torch.view_as_complex(spec)\n",
    "    scaled = complex_spec * (1.0 / (freqs ** decay_power))\n",
    "    img = torch.fft.irfft2(scaled, s=(spec.shape[2], (spec.shape[3]-1)*2), norm='ortho')\n",
    "    # Center per-channel to prevent DC drift; map to [0,1] gently\n",
    "    img = img - img.mean(dim=(-2, -1), keepdim=True)\n",
    "    img = torch.sigmoid(img * contrast)\n",
    "    return img\n",
    "\n",
    "# ===== Core priors =====\n",
    "def tv_l2(x):\n",
    "    dx = x[..., 1:] - x[..., :-1]\n",
    "    dy = x[..., :, 1:, :] - x[..., :, :-1, :]\n",
    "    return (dx.pow(2).mean() + dy.pow(2).mean())\n",
    "\n",
    "def gaussian_kernel1d(sigma):\n",
    "    radius = max(1, int(round(3*sigma)))\n",
    "    x = torch.arange(-radius, radius+1, device=device, dtype=torch.float32)\n",
    "    k = torch.exp(-0.5 * (x/sigma)**2)\n",
    "    k = (k / k.sum()).view(1, 1, -1)  # [1,1,K]\n",
    "    return k, radius\n",
    "\n",
    "def gaussian_blur(x, sigma):\n",
    "    if sigma <= 0: return x\n",
    "    k1d, r = gaussian_kernel1d(sigma)\n",
    "    # depthwise separable blur (reflect pad)\n",
    "    # Horizontal\n",
    "    w_h = k1d.view(1,1,1,-1).repeat(x.shape[1],1,1,1)\n",
    "    v = F.conv2d(F.pad(x, (r, r, 0, 0), mode='reflect'), w_h, groups=x.shape[1])\n",
    "    # Vertical\n",
    "    w_v = k1d.view(1,1,-1,1).repeat(x.shape[1],1,1,1)\n",
    "    v = F.conv2d(F.pad(v, (0, 0, r, r), mode='reflect'), w_v, groups=x.shape[1])\n",
    "    return v\n",
    "\n",
    "def corr_loss(x, sigma=1.2):\n",
    "    # Encourage x to equal its blurred version -> neighbor correlation\n",
    "    xb = gaussian_blur(x, sigma)\n",
    "    return ((x - xb)**2).mean()\n",
    "\n",
    "def laplacian_energy(x):\n",
    "    # Penalize second derivatives (gentler than TV-L1)\n",
    "    lap = torch.tensor([[0,1,0],[1,-4,1],[0,1,0]], dtype=torch.float32, device=device)\n",
    "    w = lap.view(1,1,3,3).repeat(x.shape[1],1,1,1)\n",
    "    y = F.conv2d(F.pad(x, (1,1,1,1), mode='reflect'), w, groups=x.shape[1])\n",
    "    return (y.pow(2).mean())\n",
    "\n",
    "# ===== Transform-robust multi-view =====\n",
    "def normalize_for_model(v):\n",
    "    # GoogLeNet with weights will transform internally; skip external norm.\n",
    "    return v if getattr(net, 'transform_input', False) else (v - IMAGENET_MEAN) / IMAGENET_STD\n",
    "\n",
    "def random_view(x, out_hw=(224, 224), jitter=24, rot=15, scale_range=(0.92, 1.08),\n",
    "                flip=True, noise_std=0.012):\n",
    "    # noise_std= 0.006\n",
    "    _, _, H, W = x.shape\n",
    "    ox, oy = np.random.randint(-jitter, jitter+1, 2)\n",
    "    v = torch.roll(x, shifts=(ox, oy), dims=(-1, -2))\n",
    "    ang = float(random.uniform(-rot, rot))\n",
    "    sc  = float(random.uniform(*scale_range))\n",
    "    tr  = (int(random.uniform(-0.05*W, 0.05*W)), int(random.uniform(-0.05*H, 0.05*H)))\n",
    "    v = TF.affine(v, angle=ang, translate=tr, scale=sc, shear=[0.0, 0.0])\n",
    "    if flip and random.random() < 0.5:\n",
    "        v = TF.hflip(v)\n",
    "    v = F.interpolate(v, size=out_hw, mode='bilinear', align_corners=False)\n",
    "    if noise_std > 0: v = (v + noise_std * torch.randn_like(v)).clamp(0,1)\n",
    "    return normalize_for_model(v)\n",
    "\n",
    "def cosine_anneal(start, end, t, T):\n",
    "    # Smoothly decay from start -> end over steps\n",
    "    return end + (start - end) * 0.5 * (1 + math.cos(math.pi * t / max(T,1)))\n",
    "\n",
    "# ===== Main optimization with correlation priors =====\n",
    "@torch.no_grad()\n",
    "def to_pil(x): return TF.to_pil_image(x.squeeze(0).clamp(0,1).cpu())\n",
    "\n",
    "def synthesize_class(\n",
    "    class_name=\"dumbbell\",\n",
    "    steps=1200, n_views=10, size=512, lr=0.08,\n",
    "    decay_power=2.0, contrast=1.6,\n",
    "    # Prior strengths (start -> end)\n",
    "    corr_w_start=0.03, corr_w_end=0.006, corr_sigma_start=2.0, corr_sigma_end=0.6,\n",
    "    lap_w_start=0.004, lap_w_end=0.0,\n",
    "    tv_w=5e-4, l2_w=1e-6,\n",
    "    blur_every=100\n",
    "):\n",
    "    target = class_idx_from_name(class_name)\n",
    "    spec, freqs = make_fft_params(size, size)\n",
    "    opt = torch.optim.Adam([spec], lr=lr)\n",
    "\n",
    "    for t in range(steps):\n",
    "        img = spectrum_to_image(spec, freqs, decay_power=decay_power, contrast=contrast)\n",
    "        if blur_every and t and t % blur_every == 0:\n",
    "            img = gaussian_blur(img, sigma=0.8)\n",
    "\n",
    "        # Build multi-view batch\n",
    "        batch = torch.cat([random_view(img) for _ in range(n_views)], dim=0)\n",
    "        logits = net(batch)\n",
    "        cls = logits[:, target].mean()\n",
    "\n",
    "        # Priors + schedules\n",
    "        cw  = cosine_anneal(corr_w_start, corr_w_end, t, steps)\n",
    "        lw  = cosine_anneal(lap_w_start,  lap_w_end,  t, steps)\n",
    "        cs  = cosine_anneal(corr_sigma_start, corr_sigma_end, t, steps)\n",
    "\n",
    "        loss_corr = corr_loss(img, sigma=cs)\n",
    "        loss_lap  = laplacian_energy(img)\n",
    "        loss_tv   = tv_l2(img)\n",
    "        loss_l2   = ((img - 0.5)**2).mean()\n",
    "\n",
    "        loss = cls - cw*loss_corr - lw*loss_lap - tv_w*loss_tv - l2_w*loss_l2\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        (-loss).backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Stabilizers in spectrum space\n",
    "        with torch.no_grad():\n",
    "            spec[:, :, 0, 0, :] = 0      # no DC drift\n",
    "            spec.mul_(0.9995)            # very gentle amplitude damping\n",
    "\n",
    "    final = spectrum_to_image(spec, freqs, decay_power=decay_power, contrast=contrast)\n",
    "    return to_pil(final)\n",
    "\n",
    "# Example:\n",
    "out = synthesize_class(\"dumbbell\", steps = 800, lr = 0.05, decay_power=.01, corr_w_start=0.08, corr_sigma_start = 2, blur_every=100, contrast=0.5)\n",
    "# out = synthesize_class(\"dumbbell\")\n",
    "out.save(\"dumbbell_last.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZzz90N849jH"
   },
   "outputs": [],
   "source": [
    "import math, random, numpy as np\n",
    "import torch, torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ===== Model (GoogLeNet; avoids double-normalization) =====\n",
    "weights = models.GoogLeNet_Weights.IMAGENET1K_V1\n",
    "net = models.googlenet(weights=weights, aux_logits=True).to(device).eval()\n",
    "for p in net.parameters(): p.requires_grad_(False)\n",
    "\n",
    "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406], device=device)[:, None, None]\n",
    "IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225], device=device)[:, None, None]\n",
    "CATEGORIES = weights.meta[\"categories\"]\n",
    "\n",
    "def class_idx_from_name(name, categories=CATEGORIES):\n",
    "    name = name.lower()\n",
    "    try: return categories.index(name)\n",
    "    except ValueError:\n",
    "        hits = [i for i,s in enumerate(categories) if name in s.lower()]\n",
    "        if not hits: raise ValueError(f\"'{name}' not in ImageNet categories.\")\n",
    "        return hits[0]\n",
    "\n",
    "# ===== FFT parameterization with progressive low-pass =====\n",
    "def radial_freq(h, w, device):\n",
    "    fy = torch.fft.fftfreq(h, d=1.0).to(device).reshape(h, 1)\n",
    "    fx = torch.fft.rfftfreq(w, d=1.0).to(device).reshape(1, w//2 + 1)\n",
    "    return torch.sqrt(fx*fx + fy*fy).clamp(min=1e-6)\n",
    "\n",
    "def make_fft_params(h=512, w=512):\n",
    "    spec = torch.randn(1, 3, h, w//2 + 1, 2, device=device, requires_grad=True)\n",
    "    freqs = radial_freq(h, w, device)\n",
    "    return spec, freqs\n",
    "\n",
    "def radial_lowpass_mask(freqs, cutoff, rolloff=8.0):\n",
    "    # cutoff in [0,1] relative to Nyquist; rolloff controls steepness\n",
    "    r = freqs / freqs.max()\n",
    "    return 1.0 / (1.0 + (r / max(cutoff, 1e-6))**rolloff)\n",
    "\n",
    "def spectrum_to_image(spec, freqs, decay_power=2.2, contrast=1.4,\n",
    "                      lp_cutoff=0.35, lp_rolloff=8.0):\n",
    "    complex_spec = torch.view_as_complex(spec)                               # [1,3,H,W/2+1]\n",
    "    mask = radial_lowpass_mask(freqs, cutoff=lp_cutoff, rolloff=lp_rolloff)  # [H,W/2+1]\n",
    "    scaled = complex_spec * (1.0 / (freqs ** decay_power)) * mask\n",
    "    img = torch.fft.irfft2(scaled, s=(spec.shape[2], (spec.shape[3]-1)*2), norm='ortho')\n",
    "    # Center per-channel to prevent DC drift; avoid hard saturation with gentle sigmoid\n",
    "    img = img - img.mean(dim=(-2, -1), keepdim=True)\n",
    "    img = torch.sigmoid(img * contrast)                                      # [0,1]\n",
    "    return img\n",
    "\n",
    "# ===== Priors =====\n",
    "def tv_l2(x):\n",
    "    dx = x[..., 1:] - x[..., :-1]\n",
    "    dy = x[..., :, 1:, :] - x[..., :, :-1, :]\n",
    "    return (dx.pow(2).mean() + dy.pow(2).mean())\n",
    "\n",
    "def gaussian_kernel1d(sigma):\n",
    "    radius = max(1, int(round(3*sigma)))\n",
    "    x = torch.arange(-radius, radius+1, device=device, dtype=torch.float32)\n",
    "    k = torch.exp(-0.5 * (x/sigma)**2); k = (k / k.sum()).view(1,1,-1)\n",
    "    return k, radius\n",
    "\n",
    "def gaussian_blur(x, sigma):\n",
    "    if sigma <= 0: return x\n",
    "    k1d, r = gaussian_kernel1d(sigma)\n",
    "    w_h = k1d.view(1,1,1,-1).repeat(x.shape[1],1,1,1)\n",
    "    v = F.conv2d(F.pad(x, (r,r,0,0), mode='reflect'), w_h, groups=x.shape[1])\n",
    "    w_v = k1d.view(1,1,-1,1).repeat(x.shape[1],1,1,1)\n",
    "    v = F.conv2d(F.pad(v, (0,0,r,r), mode='reflect'), w_v, groups=x.shape[1])\n",
    "    return v\n",
    "\n",
    "def corr_loss(x, sigma=1.2):   # neighbor correlation: x ≈ blurred(x)\n",
    "    xb = gaussian_blur(x, sigma)\n",
    "    return ((x - xb)**2).mean()\n",
    "\n",
    "def laplacian_energy(x):       # softer than TV-L1\n",
    "    lap = torch.tensor([[0,1,0],[1,-4,1],[0,1,0]], dtype=torch.float32, device=device)\n",
    "    w = lap.view(1,1,3,3).repeat(x.shape[1],1,1,1)\n",
    "    y = F.conv2d(F.pad(x, (1,1,1,1), mode='reflect'), w, groups=x.shape[1])\n",
    "    return (y.pow(2).mean())\n",
    "\n",
    "# --- YUV utilities for chroma regularization ---\n",
    "def rgb_to_yuv(x):\n",
    "    r,g,b = x[:,0:1], x[:,1:2], x[:,2:3]\n",
    "    y = 0.299*r + 0.587*g + 0.114*b\n",
    "    u = b - y\n",
    "    v = r - y\n",
    "    return y,u,v\n",
    "\n",
    "def chroma_saturation_loss(x, thresh=0.30):\n",
    "    # penalize only excess saturation; keep normal colors intact\n",
    "    _,u,v = rgb_to_yuv(x)\n",
    "    return (F.relu(u.abs()-thresh).pow(2).mean() + F.relu(v.abs()-thresh).pow(2).mean())\n",
    "\n",
    "def chroma_tv_loss(x):\n",
    "    _,u,v = rgb_to_yuv(x)\n",
    "    def tv(a):\n",
    "        dx = a[..., 1:] - a[..., :-1]; dy = a[..., :, 1:, :] - a[..., :, :-1, :]\n",
    "        return (dx.pow(2).mean() + dy.pow(2).mean())\n",
    "    return tv(u) + tv(v)\n",
    "\n",
    "# ===== Transform‑robust multi‑view (with mild defocus) =====\n",
    "def normalize_for_model(v):\n",
    "    return v if getattr(net, 'transform_input', False) else (v - IMAGENET_MEAN) / IMAGENET_STD\n",
    "\n",
    "def random_view(x, out_hw=(224,224), jitter=16, rot=10, scale_range=(0.94,1.06),\n",
    "                flip=True, noise_std=0.0, blur_sigma=(0.0,1.2), grayscale_p=0.15):\n",
    "    _, _, H, W = x.shape\n",
    "    ox, oy = np.random.randint(-jitter, jitter+1, 2)\n",
    "    v = torch.roll(x, shifts=(ox, oy), dims=(-1, -2))\n",
    "    ang = float(random.uniform(-rot, rot))\n",
    "    sc  = float(random.uniform(*scale_range))\n",
    "    tr  = (int(random.uniform(-0.04*W, 0.04*W)), int(random.uniform(-0.04*H, 0.04*H)))\n",
    "    v = TF.affine(v, angle=ang, translate=tr, scale=sc, shear=[0.0, 0.0])\n",
    "    if flip and random.random() < 0.5: v = TF.hflip(v)\n",
    "    # mild, random defocus per view\n",
    "    if blur_sigma and blur_sigma[1] > 0:\n",
    "        s = random.uniform(blur_sigma[0], blur_sigma[1])\n",
    "        if s > 1e-3: v = gaussian_blur(v, s)\n",
    "    v = F.interpolate(v, size=out_hw, mode='bilinear', align_corners=False)\n",
    "    if grayscale_p>0 and random.random()<grayscale_p:\n",
    "        y,_,_ = rgb_to_yuv(v); v = torch.cat([y,y,y], dim=1)\n",
    "    if noise_std>0: v = (v + noise_std * torch.randn_like(v)).clamp(0,1)\n",
    "    return normalize_for_model(v)\n",
    "\n",
    "def cosine_anneal(start, end, t, T):\n",
    "    return end + (start - end) * 0.5 * (1 + math.cos(math.pi * t / max(T,1)))\n",
    "\n",
    "# ===== Main optimization =====\n",
    "@torch.no_grad()\n",
    "def to_pil(x): return TF.to_pil_image(x.squeeze(0).clamp(0,1).cpu())\n",
    "\n",
    "def synthesize_class_natural(\n",
    "    class_name=\"dumbbell\",\n",
    "    steps=1400, n_views=12, size=512, lr=0.05,\n",
    "    # Fourier & low-pass schedule\n",
    "    decay_power=2.4, contrast_start=1.1, contrast_end=1.6,\n",
    "    lp_cutoff_start=0.20, lp_cutoff_end=0.95, lp_rolloff=10.0,\n",
    "    # Priors (start -> end)\n",
    "    corr_w_start=0.035, corr_w_end=0.008, corr_sigma_start=2.5, corr_sigma_end=0.7,\n",
    "    lap_w_start=0.005,  lap_w_end=0.0,\n",
    "    tv_w=5e-4, l2_w=1e-6,\n",
    "    chroma_sat_w_start=0.030, chroma_sat_w_end=0.006, chroma_tv_w=0.004,\n",
    "    blur_every=120\n",
    "):\n",
    "    target = class_idx_from_name(class_name)\n",
    "    spec, freqs = make_fft_params(size, size)\n",
    "    opt = torch.optim.Adam([spec], lr=lr)\n",
    "\n",
    "    for t in range(steps):\n",
    "        # progressive low-pass + contrast schedule\n",
    "        cutoff_t   = cosine_anneal(lp_cutoff_start, lp_cutoff_end, t, steps)\n",
    "        contrast_t = cosine_anneal(contrast_start, contrast_end, t, steps)\n",
    "        img = spectrum_to_image(spec, freqs, decay_power=decay_power,\n",
    "                                contrast=contrast_t, lp_cutoff=cutoff_t, lp_rolloff=lp_rolloff)\n",
    "\n",
    "        if blur_every and t and t % blur_every == 0:\n",
    "            img = gaussian_blur(img, sigma=0.8)\n",
    "\n",
    "        # multi-view objective\n",
    "        batch = torch.cat([random_view(img) for _ in range(n_views)], dim=0)\n",
    "        logits = net(batch)\n",
    "        cls = logits[:, target].mean()\n",
    "\n",
    "        # schedules\n",
    "        cw   = cosine_anneal(corr_w_start,      corr_w_end,      t, steps)\n",
    "        lw   = cosine_anneal(lap_w_start,       lap_w_end,       t, steps)\n",
    "        csig = cosine_anneal(corr_sigma_start,  corr_sigma_end,  t, steps)\n",
    "        csw  = cosine_anneal(chroma_sat_w_start, chroma_sat_w_end, t, steps)\n",
    "\n",
    "        # losses\n",
    "        loss_corr  = corr_loss(img, sigma=csig)\n",
    "        loss_lap   = laplacian_energy(img)\n",
    "        loss_tv    = tv_l2(img)\n",
    "        loss_l2    = ((img - 0.5)**2).mean()\n",
    "        loss_csat  = chroma_saturation_loss(img)   # penalize neon colors\n",
    "        loss_ctv   = chroma_tv_loss(img)           # keep chroma smooth\n",
    "\n",
    "        loss = (cls\n",
    "                - cw*loss_corr - lw*loss_lap - tv_w*loss_tv - l2_w*loss_l2\n",
    "                - csw*loss_csat - chroma_tv_w*loss_ctv)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        (-loss).backward()\n",
    "        opt.step()\n",
    "\n",
    "        # stabilize spectrum\n",
    "        with torch.no_grad():\n",
    "            spec[:, :, 0, 0, :] = 0    # remove DC drift\n",
    "            spec.mul_(0.9995)          # gentle damping\n",
    "\n",
    "    final = spectrum_to_image(spec, freqs, decay_power=decay_power,\n",
    "                              contrast=contrast_end, lp_cutoff=lp_cutoff_end, lp_rolloff=lp_rolloff)\n",
    "    return to_pil(final)\n",
    "\n",
    "# Example:\n",
    "out = synthesize_class(\"dumbbell\", steps = 1200, lr = 0.01, decay_power=.25, corr_w_start=0.08, corr_sigma_start = 2, blur_every=60, contrast=0.5)\n",
    "# out = synthesize_class(\"dumbbell\", steps = 800, lr = 0.01, decay_power=.25, corr_w_start=0.02, corr_sigma_start = 1, blur_every=100, contrast=0.5)\n",
    "# out = synthesize_class(\"dumbbell\")\n",
    "# out = synthesize_class_natural(\n",
    "#     class_name=\"dumbbell\",\n",
    "#     steps=1300, n_views=12, size=512, lr=0.01,\n",
    "#     decay_power=0.25,                     # <- key change\n",
    "#     contrast_start=1.2, contrast_end=1.6,\n",
    "#     lp_cutoff_start=0.35, lp_cutoff_end=0.95, lp_rolloff=10.0,\n",
    "#     # NEW: add to your function signature + schedule inside:\n",
    "#     # hp_cutoff_start=0.12, hp_cutoff_end=0.00,\n",
    "#     corr_w_start=0.02, corr_w_end=0.006, corr_sigma_start=2.0, corr_sigma_end=0.6,\n",
    "#     lap_w_start=0.003, lap_w_end=0.0,\n",
    "#     tv_w=2e-4, l2_w=1e-6,\n",
    "#     chroma_sat_w_start=0.012, chroma_sat_w_end=0.004, chroma_tv_w=0.003,\n",
    "#     blur_every=140\n",
    "# )\n",
    "\n",
    "out.save(\"dumbbell_natural.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3Qv5cwL98fp"
   },
   "outputs": [],
   "source": [
    "cv_presets = {\n",
    "  # 1) Balanced, natural-looking dumbbell (good baseline)\n",
    "  \"balanced_natural\": dict(\n",
    "    steps=1300, n_views=12, size=512, lr=0.05,\n",
    "    decay_power=0.8, contrast_start=1.2, contrast_end=1.6,\n",
    "    lp_cutoff_start=0.35, lp_cutoff_end=0.95, lp_rolloff=10.0,\n",
    "    corr_w_start=0.020, corr_w_end=0.006, corr_sigma_start=2.0, corr_sigma_end=0.6,\n",
    "    lap_w_start=0.003, lap_w_end=0.0,\n",
    "    tv_w=2e-4, l2_w=1e-6,\n",
    "    chroma_sat_w_start=0.012, chroma_sat_w_end=0.004, chroma_tv_w=0.003,\n",
    "    blur_every=140\n",
    "  ),\n",
    "\n",
    "  # 2) “Arms amplifier” (pushes the dataset bias harder; more anatomy-like forms)\n",
    "  \"arms_amplifier\": dict(\n",
    "    steps=1600, n_views=14, size=512, lr=0.06,\n",
    "    decay_power=0.6, contrast_start=1.1, contrast_end=1.5,\n",
    "    lp_cutoff_start=0.30, lp_cutoff_end=0.95, lp_rolloff=8.0,\n",
    "    corr_w_start=0.016, corr_w_end=0.004, corr_sigma_start=1.8, corr_sigma_end=0.6,\n",
    "    lap_w_start=0.002, lap_w_end=0.0,\n",
    "    tv_w=1.5e-4, l2_w=1e-6,\n",
    "    chroma_sat_w_start=0.010, chroma_sat_w_end=0.003, chroma_tv_w=0.002,\n",
    "    blur_every=160\n",
    "  ),\n",
    "\n",
    "  # 3) Soft pastel “dream” (hazy, gentle color; fewer neon artifacts)\n",
    "  \"pastel_soft_dream\": dict(\n",
    "    steps=1200, n_views=10, size=512, lr=0.045,\n",
    "    decay_power=1.0, contrast_start=1.0, contrast_end=1.35,\n",
    "    lp_cutoff_start=0.42, lp_cutoff_end=0.90, lp_rolloff=12.0,\n",
    "    corr_w_start=0.040, corr_w_end=0.010, corr_sigma_start=2.8, corr_sigma_end=0.8,\n",
    "    lap_w_start=0.004, lap_w_end=0.001,\n",
    "    tv_w=3e-4, l2_w=1e-6,\n",
    "    chroma_sat_w_start=0.040, chroma_sat_w_end=0.010, chroma_tv_w=0.006,\n",
    "    blur_every=80\n",
    "  ),\n",
    "\n",
    "  # 4) Shape‑first, detail‑later (big coherent forms; add detail late)\n",
    "  \"shape_then_detail\": dict(\n",
    "    steps=1500, n_views=12, size=512, lr=0.05,\n",
    "    decay_power=0.9, contrast_start=1.0, contrast_end=1.5,\n",
    "    lp_cutoff_start=0.28, lp_cutoff_end=0.95, lp_rolloff=12.0,\n",
    "    corr_w_start=0.030, corr_w_end=0.008, corr_sigma_start=2.4, corr_sigma_end=0.7,\n",
    "    lap_w_start=0.004, lap_w_end=0.001,\n",
    "    tv_w=2.5e-4, l2_w=1e-6,\n",
    "    chroma_sat_w_start=0.020, chroma_sat_w_end=0.005, chroma_tv_w=0.004,\n",
    "    blur_every=120\n",
    "  ),\n",
    "\n",
    "  # 5) Crisp(er) structure, less smoothing (expect sharper metal plates)\n",
    "  \"crisper_structure\": dict(\n",
    "    steps=1100, n_views=10, size=512, lr=0.055,\n",
    "    decay_power=0.6, contrast_start=1.2, contrast_end=1.7,\n",
    "    lp_cutoff_start=0.38, lp_cutoff_end=0.95, lp_rolloff=9.0,\n",
    "    corr_w_start=0.012, corr_w_end=0.004, corr_sigma_start=1.6, corr_sigma_end=0.6,\n",
    "    lap_w_start=0.001, lap_w_end=0.0,\n",
    "    tv_w=1.2e-4, l2_w=1e-6,\n",
    "    chroma_sat_w_start=0.010, chroma_sat_w_end=0.003, chroma_tv_w=0.002,\n",
    "    blur_every=160\n",
    "  ),\n",
    "\n",
    "  # 6) Low‑color (monochrome‑ish metal look). Requires raising grayscale in your aug.\n",
    "  # If your function doesn’t expose it, set grayscale_p=0.35 inside random_view.\n",
    "  \"low_chroma_metal\": dict(\n",
    "    steps=1400, n_views=12, size=512, lr=0.05,\n",
    "    decay_power=0.9, contrast_start=1.1, contrast_end=1.4,\n",
    "    lp_cutoff_start=0.36, lp_cutoff_end=0.92, lp_rolloff=11.0,\n",
    "    corr_w_start=0.030, corr_w_end=0.010, corr_sigma_start=2.2, corr_sigma_end=0.8,\n",
    "    lap_w_start=0.003, lap_w_end=0.0,\n",
    "    tv_w=2e-4, l2_w=1e-6,\n",
    "    chroma_sat_w_start=0.060, chroma_sat_w_end=0.020, chroma_tv_w=0.008,\n",
    "    blur_every=100\n",
    "  ),\n",
    "\n",
    "  # 7) High‑resolution poster (bigger canvas; smoother dynamics)\n",
    "  \"highres_poster_768\": dict(\n",
    "    steps=1800, n_views=10, size=768, lr=0.045,\n",
    "    decay_power=0.8, contrast_start=1.2, contrast_end=1.6,\n",
    "    lp_cutoff_start=0.30, lp_cutoff_end=0.95, lp_rolloff=10.0,\n",
    "    corr_w_start=0.026, corr_w_end=0.008, corr_sigma_start=2.2, corr_sigma_end=0.7,\n",
    "    lap_w_start=0.003, lap_w_end=0.0,\n",
    "    tv_w=2.5e-4, l2_w=1e-6,\n",
    "    chroma_sat_w_start=0.014, chroma_sat_w_end=0.005, chroma_tv_w=0.003,\n",
    "    blur_every=180\n",
    "  ),\n",
    "\n",
    "  # 8) Fine detail early (let micro‑structure appear; more risk of noise)\n",
    "  \"detail_forward\": dict(\n",
    "    steps=1200, n_views=10, size=512, lr=0.055,\n",
    "    decay_power=0.5, contrast_start=1.3, contrast_end=1.7,\n",
    "    lp_cutoff_start=0.45, lp_cutoff_end=0.95, lp_rolloff=8.0,\n",
    "    corr_w_start=0.010, corr_w_end=0.003, corr_sigma_start=1.4, corr_sigma_end=0.6,\n",
    "    lap_w_start=0.000, lap_w_end=0.0,\n",
    "    tv_w=1e-4, l2_w=1e-6,\n",
    "    chroma_sat_w_start=0.008, chroma_sat_w_end=0.003, chroma_tv_w=0.002,\n",
    "    blur_every=160\n",
    "  ),\n",
    "\n",
    "  # 9) Hard color clamp (for teaching how chroma priors tame neon)\n",
    "  \"color_clamp_strict\": dict(\n",
    "    steps=1300, n_views=12, size=512, lr=0.05,\n",
    "    decay_power=0.9, contrast_start=1.1, contrast_end=1.5,\n",
    "    lp_cutoff_start=0.34, lp_cutoff_end=0.92, lp_rolloff=12.0,\n",
    "    corr_w_start=0.028, corr_w_end=0.010, corr_sigma_start=2.4, corr_sigma_end=0.7,\n",
    "    lap_w_start=0.003, lap_w_end=0.0,\n",
    "    tv_w=2.2e-4, l2_w=1e-6,\n",
    "    chroma_sat_w_start=0.080, chroma_sat_w_end=0.025, chroma_tv_w=0.010,\n",
    "    blur_every=120\n",
    "  ),\n",
    "\n",
    "  # 10) Super‑soft focus (max dreaminess; very gentle detail)\n",
    "  \"super_soft_focus\": dict(\n",
    "    steps=1400, n_views=12, size=512, lr=0.045,\n",
    "    decay_power=1.2, contrast_start=1.0, contrast_end=1.3,\n",
    "    lp_cutoff_start=0.26, lp_cutoff_end=0.90, lp_rolloff=14.0,\n",
    "    corr_w_start=0.050, corr_w_end=0.015, corr_sigma_start=3.0, corr_sigma_end=1.0,\n",
    "    lap_w_start=0.005, lap_w_end=0.001,\n",
    "    tv_w=3e-4, l2_w=1e-6,\n",
    "    chroma_sat_w_start=0.030, chroma_sat_w_end=0.010, chroma_tv_w=0.007,\n",
    "    blur_every=80\n",
    "  ),\n",
    "\n",
    "  # 11) Low‑noise “steel” (subtle colors, mid‑high structure)\n",
    "  \"steel_low_noise\": dict(\n",
    "    steps=1200, n_views=12, size=512, lr=0.05,\n",
    "    decay_power=0.7, contrast_start=1.15, contrast_end=1.55,\n",
    "    lp_cutoff_start=0.38, lp_cutoff_end=0.95, lp_rolloff=10.0,\n",
    "    corr_w_start=0.024, corr_w_end=0.007, corr_sigma_start=2.0, corr_sigma_end=0.7,\n",
    "    lap_w_start=0.002, lap_w_end=0.0,\n",
    "    tv_w=2e-4, l2_w=1e-6,\n",
    "    chroma_sat_w_start=0.020, chroma_sat_w_end=0.006, chroma_tv_w=0.004,\n",
    "    blur_every=120\n",
    "  ),\n",
    "\n",
    "  # 12) Light‑run (good for quick comparisons of settings)\n",
    "  \"light_run\": dict(\n",
    "    steps=700, n_views=8, size=448, lr=0.055,\n",
    "    decay_power=0.8, contrast_start=1.2, contrast_end=1.6,\n",
    "    lp_cutoff_start=0.40, lp_cutoff_end=0.95, lp_rolloff=10.0,\n",
    "    corr_w_start=0.016, corr_w_end=0.006, corr_sigma_start=1.8, corr_sigma_end=0.7,\n",
    "    lap_w_start=0.002, lap_w_end=0.0,\n",
    "    tv_w=1.8e-4, l2_w=1e-6,\n",
    "    chroma_sat_w_start=0.012, chroma_sat_w_end=0.004, chroma_tv_w=0.003,\n",
    "    blur_every=120\n",
    "  )\n",
    "}\n",
    "\n",
    "for name, p in cv_presets.items():\n",
    "    out = synthesize_class_natural(\"dumbbell\", **p)\n",
    "    out.save(f\"cv_{name}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2oH8iHD_LL4z"
   },
   "outputs": [],
   "source": [
    "# --- Class-from-noise with BAND-PASS schedule (GoogLeNet + FFT) ---\n",
    "# Requirements: torch, torchvision, PIL\n",
    "\n",
    "import math, random, numpy as np\n",
    "import torch, torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# =========================\n",
    "# 1) Model (GoogLeNet)\n",
    "# =========================\n",
    "# With pretrained weights, GoogLeNet normalizes INSIDE the model (transform_input=True),\n",
    "# so we must NOT pre-normalize externally.\n",
    "weights = models.GoogLeNet_Weights.IMAGENET1K_V1\n",
    "net = models.googlenet(weights=weights, aux_logits=True).to(device).eval()\n",
    "for p in net.parameters(): p.requires_grad_(False)\n",
    "CATEGORIES = weights.meta[\"categories\"]\n",
    "\n",
    "def class_idx_from_name(name, categories=CATEGORIES):\n",
    "    name = name.lower()\n",
    "    try:\n",
    "        return categories.index(name)\n",
    "    except ValueError:\n",
    "        hits = [i for i, s in enumerate(categories) if name in s.lower()]\n",
    "        if not hits: raise ValueError(f\"'{name}' not in ImageNet categories\")\n",
    "        return hits[0]\n",
    "\n",
    "# =========================\n",
    "# 2) Fourier parameterization + band-pass masks\n",
    "# =========================\n",
    "def radial_freq(h, w, device):\n",
    "    fy = torch.fft.fftfreq(h, d=1.0).to(device).reshape(h, 1)\n",
    "    fx = torch.fft.rfftfreq(w, d=1.0).to(device).reshape(1, w//2 + 1)\n",
    "    return torch.sqrt(fx*fx + fy*fy).clamp(min=1e-6)  # avoid divide-by-zero\n",
    "\n",
    "def make_fft_params(h=512, w=512):\n",
    "    # Real/imag spectrum with grads: [1, 3, H, W//2+1, 2]\n",
    "    spec = torch.randn(1, 3, h, w//2 + 1, 2, device=device, requires_grad=True)\n",
    "    freqs = radial_freq(h, w, device)\n",
    "    return spec, freqs\n",
    "\n",
    "def radial_lowpass_mask(freqs, cutoff, rolloff=8.0):\n",
    "    # Butterworth-style LP; cutoff is relative to Nyquist in [0,1]\n",
    "    r = (freqs / freqs.max()).clamp(min=1e-6)\n",
    "    return 1.0 / (1.0 + (r / max(cutoff, 1e-6))**rolloff)\n",
    "\n",
    "def radial_highpass_mask(freqs, cutoff, rolloff=8.0):\n",
    "    # Butterworth-style HP; cutoff is relative to Nyquist in [0,1]\n",
    "    r = (freqs / freqs.max()).clamp(min=1e-6)\n",
    "    return 1.0 / (1.0 + (cutoff / r)**rolloff)\n",
    "\n",
    "def spectrum_to_image_bp(\n",
    "    spec, freqs,\n",
    "    decay_power=0.8,         # 0.5–1.0 works well with band-pass\n",
    "    contrast=1.4,\n",
    "    lp_cutoff=0.45,          # low-pass cutoff (open highs gradually)\n",
    "    hp_cutoff=0.10,          # high-pass cutoff (let lows in later)\n",
    "    rolloff=10.0\n",
    "):\n",
    "    complex_spec = torch.view_as_complex(spec)                              # [1,3,H,W/2+1]\n",
    "    lp = radial_lowpass_mask(freqs, cutoff=lp_cutoff, rolloff=rolloff)      # [H,W/2+1]\n",
    "    hp = radial_highpass_mask(freqs, cutoff=hp_cutoff, rolloff=rolloff)\n",
    "    bp = lp * hp\n",
    "    scaled = complex_spec * (1.0 / (freqs ** decay_power)) * bp\n",
    "    img = torch.fft.irfft2(scaled, s=(spec.shape[2], (spec.shape[3]-1)*2), norm='ortho')\n",
    "    # Remove per-channel DC & map to [0,1] gently (avoid tanh saturation)\n",
    "    img = img - img.mean(dim=(-2, -1), keepdim=True)\n",
    "    img = torch.sigmoid(img * contrast)\n",
    "    return img  # [1,3,H,W] in [0,1]\n",
    "\n",
    "# =========================\n",
    "# 3) Priors (correlation, Laplacian, chroma)\n",
    "# =========================\n",
    "def gaussian_kernel1d(sigma, device):\n",
    "    r = max(1, int(round(3*sigma)))\n",
    "    x = torch.arange(-r, r+1, device=device, dtype=torch.float32)\n",
    "    k = torch.exp(-0.5*(x/sigma)**2); k = (k/k.sum()).view(1,1,-1)\n",
    "    return k, r\n",
    "\n",
    "def gaussian_blur(x, sigma):\n",
    "    if sigma <= 0: return x\n",
    "    k1d, r = gaussian_kernel1d(sigma, x.device)\n",
    "    w_h = k1d.view(1,1,1,-1).repeat(x.shape[1],1,1,1)\n",
    "    v = F.conv2d(F.pad(x, (r,r,0,0), mode='reflect'), w_h, groups=x.shape[1])\n",
    "    w_v = k1d.view(1,1,-1,1).repeat(x.shape[1],1,1,1)\n",
    "    v = F.conv2d(F.pad(v, (0,0,r,r), mode='reflect'), w_v, groups=x.shape[1])\n",
    "    return v\n",
    "\n",
    "def corr_loss(x, sigma=1.2):\n",
    "    # Encourage x to equal its blurred self (neighbor correlation)\n",
    "    return ((x - gaussian_blur(x, sigma))**2).mean()\n",
    "\n",
    "def laplacian_energy(x):\n",
    "    lap = torch.tensor([[0,1,0],[1,-4,1],[0,1,0]], dtype=torch.float32, device=x.device)\n",
    "    w = lap.view(1,1,3,3).repeat(x.shape[1],1,1,1)\n",
    "    y = F.conv2d(F.pad(x, (1,1,1,1), mode='reflect'), w, groups=x.shape[1])\n",
    "    return (y.pow(2).mean())\n",
    "\n",
    "def tv_l2(x):\n",
    "    dx = x[..., 1:] - x[..., :-1]\n",
    "    dy = x[..., :, 1:, :] - x[..., :, :-1, :]\n",
    "    return (dx.pow(2).mean() + dy.pow(2).mean())\n",
    "\n",
    "def rgb_to_yuv(x):\n",
    "    r, g, b = x[:,0:1], x[:,1:2], x[:,2:3]\n",
    "    y = 0.299*r + 0.587*g + 0.114*b\n",
    "    u = b - y\n",
    "    v = r - y\n",
    "    return y, u, v\n",
    "\n",
    "def chroma_saturation_loss(x, thresh=0.30):\n",
    "    _, u, v = rgb_to_yuv(x)\n",
    "    return (F.relu(u.abs()-thresh).pow(2).mean() +\n",
    "            F.relu(v.abs()-thresh).pow(2).mean())\n",
    "\n",
    "def chroma_tv_loss(x):\n",
    "    _, u, v = rgb_to_yuv(x)\n",
    "    def tv(a):\n",
    "        dx = a[..., 1:] - a[..., :-1]\n",
    "        dy = a[..., :, 1:, :] - a[..., :, :-1, :]\n",
    "        return (dx.pow(2).mean() + dy.pow(2).mean())\n",
    "    return tv(u) + tv(v)\n",
    "\n",
    "# =========================\n",
    "# 4) Multi-view augmentation\n",
    "# =========================\n",
    "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406], device=device)[:, None, None]\n",
    "IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225], device=device)[:, None, None]\n",
    "\n",
    "def normalize_for_model(v):\n",
    "    # GoogLeNet with weights does its own normalization; skip external norm.\n",
    "    return v if getattr(net, 'transform_input', False) else (v - IMAGENET_MEAN) / IMAGENET_STD\n",
    "\n",
    "def random_view(\n",
    "    x, out_hw=(224,224),\n",
    "    jitter=16, rot=10, scale_range=(0.94,1.06),\n",
    "    flip=True, noise_std=0.0, blur_sigma=(0.0,1.2), grayscale_p=0.15\n",
    "):\n",
    "    _, _, H, W = x.shape\n",
    "    ox, oy = np.random.randint(-jitter, jitter+1, 2)\n",
    "    v = torch.roll(x, shifts=(ox, oy), dims=(-1, -2))\n",
    "    ang = float(random.uniform(-rot, rot))\n",
    "    sc  = float(random.uniform(*scale_range))\n",
    "    tr  = (int(random.uniform(-0.04*W, 0.04*W)), int(random.uniform(-0.04*H, 0.04*H)))\n",
    "    v = TF.affine(v, angle=ang, translate=tr, scale=sc, shear=[0.0, 0.0])\n",
    "    if flip and random.random() < 0.5: v = TF.hflip(v)\n",
    "    # mild, random defocus per view\n",
    "    if blur_sigma and blur_sigma[1] > 0:\n",
    "        s = random.uniform(blur_sigma[0], blur_sigma[1])\n",
    "        if s > 1e-3: v = gaussian_blur(v, s)\n",
    "    v = F.interpolate(v, size=out_hw, mode='bilinear', align_corners=False)\n",
    "    if grayscale_p>0 and random.random()<grayscale_p:\n",
    "        y,_,_ = rgb_to_yuv(v); v = torch.cat([y,y,y], dim=1)\n",
    "    if noise_std>0: v = (v + noise_std*torch.randn_like(v)).clamp(0,1)\n",
    "    return normalize_for_model(v)\n",
    "\n",
    "def cosine_anneal(start, end, t, T):\n",
    "    return end + (start - end) * 0.5 * (1 + math.cos(math.pi * t / max(T,1)))\n",
    "\n",
    "@torch.no_grad()\n",
    "def to_pil(x): return TF.to_pil_image(x.squeeze(0).clamp(0,1).cpu())\n",
    "\n",
    "# =========================\n",
    "# 5) Main function (band-pass schedules)\n",
    "# =========================\n",
    "def synthesize_class_natural_bp(\n",
    "    class_name=\"dumbbell\",\n",
    "    *,\n",
    "    steps=1300, n_views=12, size=512, lr=0.05, seed=None,\n",
    "    # FFT & band-pass schedule\n",
    "    decay_power=0.8,                 # <= key; 0.5–1.0 recommended with band-pass\n",
    "    contrast_start=1.2, contrast_end=1.6,\n",
    "    lp_cutoff_start=0.35, lp_cutoff_end=0.95,\n",
    "    hp_cutoff_start=0.12, hp_cutoff_end=0.00,\n",
    "    rolloff=10.0,\n",
    "    # Priors (start -> end)\n",
    "    corr_w_start=0.020, corr_w_end=0.006, corr_sigma_start=2.0, corr_sigma_end=0.6,\n",
    "    lap_w_start=0.003,  lap_w_end=0.0,\n",
    "    tv_w=2e-4, l2_w=1e-6,\n",
    "    chroma_sat_w_start=0.012, chroma_sat_w_end=0.004, chroma_tv_w=0.003,\n",
    "    blur_every=140,\n",
    "    # Aug params exposed for convenience\n",
    "    view_jitter=16, view_rot=10, view_scale=(0.94,1.06), grayscale_p=0.15\n",
    "):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed); np.random.seed(seed); random.seed(seed)\n",
    "\n",
    "    target = class_idx_from_name(class_name)\n",
    "    spec, freqs = make_fft_params(size, size)\n",
    "    opt = torch.optim.Adam([spec], lr=lr)\n",
    "\n",
    "    for t in range(steps):\n",
    "        # Band-pass + contrast schedules\n",
    "        lp_t = cosine_anneal(lp_cutoff_start, lp_cutoff_end, t, steps)\n",
    "        hp_t = cosine_anneal(hp_cutoff_start, hp_cutoff_end, t, steps)\n",
    "        contrast_t = cosine_anneal(contrast_start, contrast_end, t, steps)\n",
    "\n",
    "        img = spectrum_to_image_bp(\n",
    "            spec, freqs,\n",
    "            decay_power=decay_power, contrast=contrast_t,\n",
    "            lp_cutoff=lp_t, hp_cutoff=hp_t, rolloff=rolloff\n",
    "        )\n",
    "\n",
    "        if blur_every and t and t % blur_every == 0:\n",
    "            img = gaussian_blur(img, sigma=0.8)\n",
    "\n",
    "        # Transform-robust objective: average class logit over random views\n",
    "        batch = torch.cat([\n",
    "            random_view(\n",
    "                img, out_hw=(224,224),\n",
    "                jitter=view_jitter, rot=view_rot, scale_range=view_scale,\n",
    "                blur_sigma=(0.0,1.2), grayscale_p=grayscale_p\n",
    "            )\n",
    "            for _ in range(n_views)\n",
    "        ], dim=0)\n",
    "        logits = net(batch)\n",
    "        cls = logits[:, target].mean()\n",
    "\n",
    "        # Priors + schedules\n",
    "        cw   = cosine_anneal(corr_w_start, corr_w_end, t, steps)\n",
    "        lw   = cosine_anneal(lap_w_start,  lap_w_end,  t, steps)\n",
    "        csig = cosine_anneal(corr_sigma_start, corr_sigma_end, t, steps)\n",
    "        csw  = cosine_anneal(chroma_sat_w_start, chroma_sat_w_end, t, steps)\n",
    "\n",
    "        loss_corr = corr_loss(img, sigma=csig)\n",
    "        loss_lap  = laplacian_energy(img)\n",
    "        loss_tv   = tv_l2(img)\n",
    "        loss_l2   = ((img - 0.5)**2).mean()\n",
    "        loss_csat = chroma_saturation_loss(img)  # clamp neon\n",
    "        loss_ctv  = chroma_tv_loss(img)          # smooth chroma\n",
    "\n",
    "        loss = (cls\n",
    "                - cw*loss_corr - lw*loss_lap - tv_w*loss_tv - l2_w*loss_l2\n",
    "                - csw*loss_csat - chroma_tv_w*loss_ctv)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        (-loss).backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Stabilize spectrum\n",
    "        with torch.no_grad():\n",
    "            spec[:, :, 0, 0, :] = 0    # no DC drift\n",
    "            spec.mul_(0.9995)          # gentle amplitude damping\n",
    "\n",
    "    # Final image at relaxed band-pass\n",
    "    final = spectrum_to_image_bp(\n",
    "        spec, freqs,\n",
    "        decay_power=decay_power, contrast=contrast_end,\n",
    "        lp_cutoff=lp_cutoff_end, hp_cutoff=hp_cutoff_end, rolloff=rolloff\n",
    "    )\n",
    "    return to_pil(final)\n",
    "\n",
    "# =========================\n",
    "# 6) Example usage\n",
    "# =========================\n",
    "out = synthesize_class_natural_bp(\n",
    "    \"dumbbell\",\n",
    "    steps=1200, n_views=12, size=512, lr=0.01, seed=0,\n",
    "    decay_power=0.1,\n",
    "    contrast_start=1.2, contrast_end=1.6,\n",
    "    lp_cutoff_start=0.35, lp_cutoff_end=0.95,\n",
    "    hp_cutoff_start=0.12, hp_cutoff_end=0.00,\n",
    "    rolloff=10.0,\n",
    "    corr_w_start=0.020, corr_w_end=0.006, corr_sigma_start=2.0, corr_sigma_end=0.6,\n",
    "    lap_w_start=0.003,  lap_w_end=0.0,\n",
    "    tv_w=2e-4, l2_w=1e-6,\n",
    "    chroma_sat_w_start=0.012, chroma_sat_w_end=0.004, chroma_tv_w=0.003,\n",
    "    blur_every=100,\n",
    ")\n",
    "out.save(\"dumbbell_bp.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "FwYQ47JuRuQ6",
    "outputId": "efdfba3f-5617-4272-9d33-4c8352e560d1"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "synthesize_class_natural_bp() got an unexpected keyword argument 'lp_rolloff'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-4236591949.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcv_bp_presets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynthesize_class_natural_bp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dumbbell\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cv_{name}.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: synthesize_class_natural_bp() got an unexpected keyword argument 'lp_rolloff'"
     ]
    }
   ],
   "source": [
    "cv_bp_presets = {\n",
    "  # Keeps mid-frequencies early, opens both ends gradually (great for structure)\n",
    "  \"bp_midband_balanced\": dict(\n",
    "    steps=1300, n_views=12, size=512, lr=0.05,\n",
    "    decay_power=0.8, contrast_start=1.2, contrast_end=1.6,\n",
    "    lp_cutoff_start=0.35, lp_cutoff_end=0.95, hp_cutoff_start=0.12, hp_cutoff_end=0.00, lp_rolloff=10.0,\n",
    "    corr_w_start=0.020, corr_w_end=0.006, corr_sigma_start=2.0, corr_sigma_end=0.6,\n",
    "    lap_w_start=0.003, lap_w_end=0.0,\n",
    "    tv_w=2e-4, l2_w=1e-6,\n",
    "    chroma_sat_w_start=0.012, chroma_sat_w_end=0.004, chroma_tv_w=0.003,\n",
    "    blur_every=140\n",
    "  ),\n",
    "\n",
    "  # Strong shape bias (blocks very low & very high freqs initially)\n",
    "  \"bp_shape_first\": dict(\n",
    "    steps=1500, n_views=12, size=512, lr=0.05,\n",
    "    decay_power=0.9, contrast_start=1.1, contrast_end=1.5,\n",
    "    lp_cutoff_start=0.32, lp_cutoff_end=0.95, hp_cutoff_start=0.18, hp_cutoff_end=0.02, lp_rolloff=12.0,\n",
    "    corr_w_start=0.030, corr_w_end=0.008, corr_sigma_start=2.4, corr_sigma_end=0.7,\n",
    "    lap_w_start=0.004, lap_w_end=0.001,\n",
    "    tv_w=2.2e-4, l2_w=1e-6,\n",
    "    chroma_sat_w_start=0.016, chroma_sat_w_end=0.005, chroma_tv_w=0.004,\n",
    "    blur_every=120\n",
    "  ),\n",
    "\n",
    "  # Detail early, but still controlled (less neon than pure low-pass)\n",
    "  \"bp_detail_forward\": dict(\n",
    "    steps=1200, n_views=10, size=512, lr=0.055,\n",
    "    decay_power=0.6, contrast_start=1.3, contrast_end=1.7,\n",
    "    lp_cutoff_start=0.45, lp_cutoff_end=0.95, hp_cutoff_start=0.08, hp_cutoff_end=0.00, lp_rolloff=8.0,\n",
    "    corr_w_start=0.012, corr_w_end=0.004, corr_sigma_start=1.6, corr_sigma_end=0.6,\n",
    "    lap_w_start=0.001, lap_w_end=0.0,\n",
    "    tv_w=1.2e-4, l2_w=1e-6,\n",
    "    chroma_sat_w_start=0.010, chroma_sat_w_end=0.003, chroma_tv_w=0.002,\n",
    "    blur_every=160\n",
    "  ),\n",
    "\n",
    "  # Pastel dream with band-pass\n",
    "  \"bp_pastel\": dict(\n",
    "    steps=1200, n_views=10, size=512, lr=0.045,\n",
    "    decay_power=1.0, contrast_start=1.0, contrast_end=1.35,\n",
    "    lp_cutoff_start=0.40, lp_cutoff_end=0.90, hp_cutoff_start=0.15, hp_cutoff_end=0.02, lp_rolloff=12.0,\n",
    "    corr_w_start=0.040, corr_w_end=0.010, corr_sigma_start=2.8, corr_sigma_end=0.8,\n",
    "    lap_w_start=0.004, lap_w_end=0.001,\n",
    "    tv_w=3e-4, l2_w=1e-6,\n",
    "    chroma_sat_w_start=0.040, chroma_sat_w_end=0.010, chroma_tv_w=0.006,\n",
    "    blur_every=100\n",
    "  ),\n",
    "\n",
    "  # Punchy color but still controlled (looser chroma clamp)\n",
    "  \"bp_vivid\": dict(\n",
    "    steps=1100, n_views=10, size=512, lr=0.055,\n",
    "    decay_power=0.7, contrast_start=1.25, contrast_end=1.75,\n",
    "    lp_cutoff_start=0.42, lp_cutoff_end=0.95, hp_cutoff_start=0.10, hp_cutoff_end=0.00, lp_rolloff=9.0,\n",
    "    corr_w_start=0.010, corr_w_end=0.003, corr_sigma_start=1.6, corr_sigma_end=0.6,\n",
    "    lap_w_start=0.001, lap_w_end=0.0,\n",
    "    tv_w=1.2e-4, l2_w=1e-6,\n",
    "    chroma_sat_w_start=0.008, chroma_sat_w_end=0.003, chroma_tv_w=0.002,\n",
    "    blur_every=160\n",
    "  ),\n",
    "\n",
    "  # Big poster canvas with band-pass\n",
    "  \"bp_highres_768\": dict(\n",
    "    steps=1800, n_views=10, size=768, lr=0.045,\n",
    "    decay_power=0.8, contrast_start=1.2, contrast_end=1.6,\n",
    "    lp_cutoff_start=0.32, lp_cutoff_end=0.95, hp_cutoff_start=0.14, hp_cutoff_end=0.00, lp_rolloff=10.0,\n",
    "    corr_w_start=0.026, corr_w_end=0.008, corr_sigma_start=2.2, corr_sigma_end=0.7,\n",
    "    lap_w_start=0.003, lap_w_end=0.0,\n",
    "    tv_w=2.5e-4, l2_w=1e-6,\n",
    "    chroma_sat_w_start=0.014, chroma_sat_w_end=0.005, chroma_tv_w=0.003,\n",
    "    blur_every=180\n",
    "  )\n",
    "}\n",
    "\n",
    "for name, p in cv_bp_presets.items():\n",
    "    out = synthesize_class_natural_bp(\"dumbbell\", **p)\n",
    "    out.save(f\"cv_{name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ce30FN6JSMiW",
    "outputId": "62c50b53-e1ef-4627-cc2b-9788c9dba379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote deepdream_dumbbells.png\n"
     ]
    }
   ],
   "source": [
    "# deepdream_dumbbells.py\n",
    "import math, torch, torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms.functional import resize\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "# ---- Config ----\n",
    "DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL        = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1).to(DEVICE).eval()\n",
    "TARGET_CLASS = 543  # \"dumbbell\" in ImageNet-1k mapping\n",
    "STEPS        = 600   # per octave\n",
    "STEP_SIZE    = 0.01\n",
    "TV_WEIGHT    = 1e-4\n",
    "NUM_OCTAVES  = 5\n",
    "OCTAVE_SCALE = 1.35\n",
    "JITTER       = 16\n",
    "SEED         = 3\n",
    "\n",
    "# InceptionV3 expects 299x299 and specific normalization\n",
    "preproc = models.Inception_V3_Weights.IMAGENET1K_V1.transforms()\n",
    "mean = torch.tensor([0.485, 0.456, 0.406], device=DEVICE).view(1,3,1,1)\n",
    "std  = torch.tensor([0.229, 0.224, 0.225], device=DEVICE).view(1,3,1,1)\n",
    "\n",
    "g = torch.Generator(device=DEVICE).manual_seed(SEED)\n",
    "# start from noise near “natural” stats\n",
    "h, w = 299, 299\n",
    "img = torch.clamp(torch.randn(1,3,h,w, device=DEVICE, generator=g)*0.2 + 0.5, 0, 1).requires_grad_(True)\n",
    "\n",
    "def tv_loss(x):\n",
    "    dx = x[:,:,1:,:] - x[:,:,:-1,:]\n",
    "    dy = x[:,:,:,1:] - x[:,:,:,:-1]\n",
    "    return (dx.pow(2).mean() + dy.pow(2).mean())\n",
    "\n",
    "@torch.no_grad()\n",
    "def to_pil(x):\n",
    "    x = torch.clamp(x, 0, 1).mul(255).byte().squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "    return Image.fromarray(x)\n",
    "\n",
    "def ascend(img, steps):\n",
    "    for _ in range(steps):\n",
    "        # jitter\n",
    "        ox = int(torch.randint(-JITTER, JITTER+1, (1,), generator=g, device=DEVICE))\n",
    "        oy = int(torch.randint(-JITTER, JITTER+1, (1,), generator=g, device=DEVICE))\n",
    "        img.data = torch.roll(torch.roll(img.data, shifts=ox, dims=2), shifts=oy, dims=3)\n",
    "\n",
    "        # forward\n",
    "        x = (img - mean)/std\n",
    "        logits = MODEL(x)\n",
    "        if isinstance(logits, tuple):  # inception_v3 returns (logits, aux) sometimes\n",
    "            logits = logits[0]\n",
    "        score = logits[:, TARGET_CLASS].mean()\n",
    "        loss = -score + TV_WEIGHT*tv_loss(img)\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            gnorm = img.grad.norm().clamp(min=1e-8)\n",
    "            img += STEP_SIZE * img.grad / gnorm\n",
    "            img.grad.zero_()\n",
    "\n",
    "        # undo jitter\n",
    "        img.data = torch.roll(torch.roll(img.data, shifts=-ox, dims=2), shifts=-oy, dims=3)\n",
    "        img.data.clamp_(0,1)\n",
    "    return img\n",
    "\n",
    "# Multi-octave (coarse→fine)\n",
    "for o in range(NUM_OCTAVES):\n",
    "    img = ascend(img, STEPS)\n",
    "    # upscale for next octave\n",
    "    if o < NUM_OCTAVES-1:\n",
    "        nh, nw = int(h*(OCTAVE_SCALE**(o+1))), int(w*(OCTAVE_SCALE**(o+1)))\n",
    "        with torch.no_grad():\n",
    "            img = resize(img, [nh, nw], antialias=True).requires_grad_(True)\n",
    "\n",
    "# Save multiple crops (to mimic the grid in the blog)\n",
    "grid = []\n",
    "base = img.detach()\n",
    "for k in range(9):\n",
    "    # Small random crop back to 299x299 and short refinement\n",
    "    with torch.no_grad():\n",
    "        H, W = base.shape[-2:]\n",
    "        top  = int(torch.randint(0, max(1, H-299), (1,), generator=g, device=DEVICE))\n",
    "        left = int(torch.randint(0, max(1, W-299), (1,), generator=g, device=DEVICE))\n",
    "        crop = base[..., top:top+299, left:left+299].clone().requires_grad_(True)\n",
    "    crop = ascend(crop, steps=20)\n",
    "    grid.append(to_pil(crop))\n",
    "\n",
    "# Make a simple 3x3 grid\n",
    "def make_grid(images, rows=3, cols=3, pad=4):\n",
    "    w, h = images[0].size\n",
    "    canvas = Image.new(\"RGB\", (cols*w + (cols-1)*pad, rows*h + (rows-1)*pad), (255,255,255))\n",
    "    i = 0\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            canvas.paste(images[i], (c*(w+pad), r*(h+pad)))\n",
    "            i += 1\n",
    "    return canvas\n",
    "\n",
    "grid_img = make_grid(grid, 3, 3, 6)\n",
    "grid_img.save(\"deepdream_dumbbells.png\")\n",
    "print(\"Wrote deepdream_dumbbells.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLUogrMBX7dp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
